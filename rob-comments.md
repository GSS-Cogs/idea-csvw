* We need to include unit multipler here as well - we’re not using it yet but we’re going to have to at some point soon <https://github.com/GSS-Cogs/idea-csvw/blob/ad108ffbaee1e174401b91bea06a1aa384dfca06/out/example.json#L30>
* I like that you’re specifying the `component_type` so we can easily switch on it. I still think the Dimension/Attributes/Measures split would be tidier and would enable the JSON schema intellisense to work a bit better.
* I think I’ve mentioned it before but I don’t think `component_uri` is the best term. We already have components in the qb/SDMX specification and this isn’t the URI of one so it’s possible we’ll end up confusing ourselves.
* I think `“codelist”: false` should be the default. It would enable users to onboard quicker and get their data out there without having to worry about codelists. Codelists add functionality but I don’t think they should be assumed by default.
* My thoughts are that we need to move towards something where we can inherit from predefined column definitions which are hosted by COGS. For instance, can I just specify that I want to extend the `gss-dimensions:referencePeriod` column definition and it automatically figures out that it has a codelist and does all the mapping for me?
  * This would be really useful when we get to things like datetimes, we could extend that column and it could be treated differently in code, looking for the format (e.g. YYYY-mm-dd) and then generating the appropriate reference period URI for me?
  * It would also be quite useful if it could do the same for other things like NUTS codes. So long as I had the right values in my column, I wouldn’t have to think about codelists and things like that.
* I don’t see how this schema would work with multi-measure cubes. Basically we need to define the list of measures as the data-set level, but with this approach you’d need to define the measure as something like `http://gss-data.org.uk/def/measure/{measure_type_column}` which would mean we don’t know which possible values the measure could take. I mean we could look at the CSV and pull out the unique measure values, but it would be good to make the JSON schema file independent of the actual CSV content so that dataproducers just need to replace the CSV file to update the data (and don’t need to mess around regenerating the JSON schema file). The same would be true for (hypothetical) multi-unit datasets.
* I still believe we should have a way of specifying the codelist metadata inside the info.json file so we don’t leave users having to wrangle with the nasty JSON schema files we currently have to work with. It would make the metadata much eaiser to manage and we could bin the codelist-manager. I think the codelist JSON schema files should be generated at the same time as the dataset’s JSON schema file in the dataset building process. This approach would also mean we can bundle together the dataset and codelist schema definitions in one JSON schema file which would mean there’s just one file to download to get all the metadata for the dataset.
* I like that you’ve started to implement some validation. I think we need to provide a tighter feedback loop so users can learn by iterative experimentation without having to come back to us and ask questions - people are going to get fed up if it takes them 3 days to get an answer to all of their questions. Users should be able to build, be warned about potential issues and then be able to validate their CSV-W (including all the SPARQL tests) locally without having to contact us.
* We’re missing the DCAT dataset metadata from the metadata JSON file. This is currently output into the `.trig` file but we need to bring it into the JSON file at some point soon. (edited) 